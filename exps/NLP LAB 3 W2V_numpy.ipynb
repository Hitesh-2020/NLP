{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5ffQ/RkVzsTHhEqWtBdbg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Implementation of word2vec using numpy."],"metadata":{"id":"ddrNqEo7dzn1"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spVwzvkfdurG","executionInfo":{"status":"ok","timestamp":1701100652082,"user_tz":-330,"elapsed":6,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"9fe2c465-73e7-4133-c973-869c3291fb12"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 45.21946973215675\n","Epoch 2/100, Loss: 44.71142553882151\n","Epoch 3/100, Loss: 44.257110838619106\n","Epoch 4/100, Loss: 43.84823830693104\n","Epoch 5/100, Loss: 43.47806044861758\n","Epoch 6/100, Loss: 43.14101130997983\n","Epoch 7/100, Loss: 42.83244986013917\n","Epoch 8/100, Loss: 42.54847204231621\n","Epoch 9/100, Loss: 42.28577017477153\n","Epoch 10/100, Loss: 42.041525657244144\n","Epoch 11/100, Loss: 41.8133255605026\n","Epoch 12/100, Loss: 41.599096666817594\n","Epoch 13/100, Loss: 41.39705249729026\n","Epoch 14/100, Loss: 41.205650178182196\n","Epoch 15/100, Loss: 41.02355489168032\n","Epoch 16/100, Loss: 40.849610271099685\n","Epoch 17/100, Loss: 40.68281352877264\n","Epoch 18/100, Loss: 40.52229440694553\n","Epoch 19/100, Loss: 40.3672972576538\n","Epoch 20/100, Loss: 40.21716571339691\n","Epoch 21/100, Loss: 40.07132952452147\n","Epoch 22/100, Loss: 39.929293223893595\n","Epoch 23/100, Loss: 39.79062634321742\n","Epoch 24/100, Loss: 39.65495495415733\n","Epoch 25/100, Loss: 39.52195434538573\n","Epoch 26/100, Loss: 39.39134267670928\n","Epoch 27/100, Loss: 39.26287547557878\n","Epoch 28/100, Loss: 39.13634086101371\n","Epoch 29/100, Loss: 39.011555396312296\n","Epoch 30/100, Loss: 38.88836048561916\n","Epoch 31/100, Loss: 38.766619241028586\n","Epoch 32/100, Loss: 38.64621375681482\n","Epoch 33/100, Loss: 38.52704273589976\n","Epoch 34/100, Loss: 38.409019421024745\n","Epoch 35/100, Loss: 38.29206978946533\n","Epoch 36/100, Loss: 38.17613097565783\n","Epoch 37/100, Loss: 38.061149890912574\n","Epoch 38/100, Loss: 37.947082013566124\n","Epoch 39/100, Loss: 37.8338903265557\n","Epoch 40/100, Loss: 37.721544382553894\n","Epoch 41/100, Loss: 37.6100194795407\n","Epoch 42/100, Loss: 37.49929593206509\n","Epoch 43/100, Loss: 37.38935842550695\n","Epoch 44/100, Loss: 37.280195442432316\n","Epoch 45/100, Loss: 37.17179875167511\n","Epoch 46/100, Loss: 37.06416295210934\n","Epoch 47/100, Loss: 36.957285064224024\n","Epoch 48/100, Loss: 36.851164163602135\n","Epoch 49/100, Loss: 36.745801051257\n","Epoch 50/100, Loss: 36.641197956511206\n","Epoch 51/100, Loss: 36.53735826873268\n","Epoch 52/100, Loss: 36.43428629478061\n","Epoch 53/100, Loss: 36.3319870394767\n","Epoch 54/100, Loss: 36.23046600681056\n","Epoch 55/100, Loss: 36.12972901992454\n","Epoch 56/100, Loss: 36.02978205821031\n","Epoch 57/100, Loss: 35.9306311100903\n","Epoch 58/100, Loss: 35.83228204026389\n","Epoch 59/100, Loss: 35.73474047036955\n","Epoch 60/100, Loss: 35.63801167215919\n","Epoch 61/100, Loss: 35.54210047240111\n","Epoch 62/100, Loss: 35.44701116882721\n","Epoch 63/100, Loss: 35.35274745652189\n","Epoch 64/100, Loss: 35.2593123642155\n","Epoch 65/100, Loss: 35.16670819999916\n","Epoch 66/100, Loss: 35.07493650601793\n","Epoch 67/100, Loss: 34.983998021733086\n","Epoch 68/100, Loss: 34.893892655366976\n","Epoch 69/100, Loss: 34.80461946316257\n","Epoch 70/100, Loss: 34.71617663610104\n","Epoch 71/100, Loss: 34.62856149372904\n","Epoch 72/100, Loss: 34.54177048475042\n","Epoch 73/100, Loss: 34.45579919403975\n","Epoch 74/100, Loss: 34.37064235573287\n","Epoch 75/100, Loss: 34.2862938720487\n","Epoch 76/100, Loss: 34.202746837492874\n","Epoch 77/100, Loss: 34.119993568090194\n","Epoch 78/100, Loss: 34.03802563528974\n","Epoch 79/100, Loss: 33.95683390418259\n","Epoch 80/100, Loss: 33.87640857566939\n","Epoch 81/100, Loss: 33.79673923221293\n","Epoch 82/100, Loss: 33.71781488680918\n","Epoch 83/100, Loss: 33.6396240348101\n","Epoch 84/100, Loss: 33.56215470823167\n","Epoch 85/100, Loss: 33.48539453218277\n","Epoch 86/100, Loss: 33.40933078305261\n","Epoch 87/100, Loss: 33.3339504480986\n","Epoch 88/100, Loss: 33.25924028608067\n","Epoch 89/100, Loss: 33.18518688859447\n","Epoch 90/100, Loss: 33.111776741761815\n","Epoch 91/100, Loss: 33.03899628794429\n","Epoch 92/100, Loss: 32.96683198715412\n","Epoch 93/100, Loss: 32.895270377845556\n","Epoch 94/100, Loss: 32.82429813677894\n","Epoch 95/100, Loss: 32.75390213766013\n","Epoch 96/100, Loss: 32.68406950826862\n","Epoch 97/100, Loss: 32.61478768579867\n","Epoch 98/100, Loss: 32.54604447014954\n","Epoch 99/100, Loss: 32.477828074913255\n","Epoch 100/100, Loss: 32.41012717582073\n","\n","Vector for 'sample': [ 0.52910913  1.12573651  0.80716836  0.03480725  0.37525336 -0.20630512\n","  1.02043301  0.57187761  0.34405546  0.6980602 ]\n"]}],"source":["import numpy as np\n","from collections import defaultdict\n","\n","class Word2Vec:\n","    def __init__(self, corpus, embedding_dim, window_size=2, learning_rate=0.01):\n","        self.corpus = corpus\n","        self.embedding_dim = embedding_dim\n","        self.window_size = window_size\n","        self.learning_rate = learning_rate\n","        self.word_index = {}\n","        self.index_word = {}\n","        self.vocab_size = 0\n","        self.data = []\n","        self.build_vocab()\n","        self.init_weights()\n","\n","    def build_vocab(self):\n","        unique_words = list(set(self.corpus))\n","        self.vocab_size = len(unique_words)\n","        for i, word in enumerate(unique_words):\n","            self.word_index[word] = i\n","            self.index_word[i] = word\n","\n","        for i, target_word in enumerate(self.corpus):\n","            target_index = self.word_index[target_word]\n","            context_words = self.get_context_words(i)\n","            for context_word in context_words:\n","                context_index = self.word_index[context_word]\n","                self.data.append((target_index, context_index))\n","\n","    def init_weights(self):\n","        self.W1 = np.random.rand(self.vocab_size, self.embedding_dim)\n","        self.W2 = np.random.rand(self.embedding_dim, self.vocab_size)\n","\n","    def get_context_words(self, target_index):\n","        context_words = []\n","        start = max(0, target_index - self.window_size)\n","        end = min(len(self.corpus), target_index + self.window_size + 1)\n","        for i in range(start, end):\n","            if i != target_index:\n","                context_words.append(self.corpus[i])\n","        return context_words\n","\n","    def softmax(self, x):\n","        exp_x = np.exp(x - np.max(x))\n","        return exp_x / exp_x.sum(axis=0, keepdims=True)\n","\n","    def forward_pass(self, target_index):\n","        target_vector = self.W1[target_index]\n","        u = np.dot(self.W2.T, target_vector)\n","        y_pred = self.softmax(u)\n","        return y_pred, target_vector\n","\n","    def backward_pass(self, target_index, context_index, y_pred, target_vector):\n","        y_true = np.zeros(self.vocab_size)\n","        y_true[context_index] = 1\n","\n","        e = y_pred - y_true\n","        dW2 = np.outer(target_vector, e)\n","        dW1 = np.dot(self.W2, e)\n","\n","        self.W1[target_index] -= self.learning_rate * dW1\n","        self.W2 -= self.learning_rate * dW2\n","\n","    def train(self, epochs):\n","        for epoch in range(epochs):\n","            loss = 0\n","            for target_index, context_index in self.data:\n","                y_pred, target_vector = self.forward_pass(target_index)\n","                loss += -np.log(y_pred[context_index])\n","                self.backward_pass(target_index, context_index, y_pred, target_vector)\n","            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n","\n","    def get_word_vector(self, word):\n","        if word in self.word_index:\n","            return self.W1[self.word_index[word]]\n","        else:\n","            return None\n","\n","# Example Usage\n","corpus = [\"this\", \"is\", \"a\", \"sample\", \"corpus\", \"for\", \"word2vec\"]\n","embedding_dim = 10\n","\n","word2vec_model = Word2Vec(corpus, embedding_dim)\n","word2vec_model.train(epochs=100)\n","\n","# Get the word vector for a specific word\n","word_vector = word2vec_model.get_word_vector(\"sample\")\n","print(f\"\\nVector for 'sample': {word_vector}\")\n"]}]}