{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOm03tUIte2ybT6el8DifoW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Implement GloVe using tensorflow gradient descent"],"metadata":{"id":"yHZ1xFa4Bg7T"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqYijpuBBdxp","executionInfo":{"status":"ok","timestamp":1701075969051,"user_tz":-330,"elapsed":5249,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"51d72202-017b-4e75-eda5-d75ce095015a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 1s 514ms/step - loss: 1.0016\n","Epoch 2/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9986\n","Epoch 3/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9956\n","Epoch 4/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9925\n","Epoch 5/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9895\n","Epoch 6/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9865\n","Epoch 7/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9834\n","Epoch 8/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9803\n","Epoch 9/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9772\n","Epoch 10/100\n","1/1 [==============================] - 0s 9ms/step - loss: 0.9740\n","Epoch 11/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9708\n","Epoch 12/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9675\n","Epoch 13/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9641\n","Epoch 14/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9607\n","Epoch 15/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9572\n","Epoch 16/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9535\n","Epoch 17/100\n","1/1 [==============================] - 0s 5ms/step - loss: 0.9498\n","Epoch 18/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9460\n","Epoch 19/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9420\n","Epoch 20/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9380\n","Epoch 21/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9338\n","Epoch 22/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9294\n","Epoch 23/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9249\n","Epoch 24/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9203\n","Epoch 25/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.9155\n","Epoch 26/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.9105\n","Epoch 27/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.9054\n","Epoch 28/100\n","1/1 [==============================] - 0s 9ms/step - loss: 0.9001\n","Epoch 29/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8946\n","Epoch 30/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8890\n","Epoch 31/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8831\n","Epoch 32/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.8771\n","Epoch 33/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8708\n","Epoch 34/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8644\n","Epoch 35/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.8578\n","Epoch 36/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8509\n","Epoch 37/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8439\n","Epoch 38/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8366\n","Epoch 39/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8291\n","Epoch 40/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8214\n","Epoch 41/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.8135\n","Epoch 42/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.8054\n","Epoch 43/100\n","1/1 [==============================] - 0s 11ms/step - loss: 0.7971\n","Epoch 44/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.7885\n","Epoch 45/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.7798\n","Epoch 46/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.7708\n","Epoch 47/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.7616\n","Epoch 48/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.7522\n","Epoch 49/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.7426\n","Epoch 50/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.7328\n","Epoch 51/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.7228\n","Epoch 52/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.7126\n","Epoch 53/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.7022\n","Epoch 54/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6916\n","Epoch 55/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6809\n","Epoch 56/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6699\n","Epoch 57/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6588\n","Epoch 58/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6476\n","Epoch 59/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6361\n","Epoch 60/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6246\n","Epoch 61/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6129\n","Epoch 62/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6010\n","Epoch 63/100\n","1/1 [==============================] - 0s 11ms/step - loss: 0.5890\n","Epoch 64/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.5769\n","Epoch 65/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.5648\n","Epoch 66/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.5525\n","Epoch 67/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.5401\n","Epoch 68/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.5276\n","Epoch 69/100\n","1/1 [==============================] - 0s 5ms/step - loss: 0.5151\n","Epoch 70/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.5025\n","Epoch 71/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.4899\n","Epoch 72/100\n","1/1 [==============================] - 0s 8ms/step - loss: 0.4773\n","Epoch 73/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.4646\n","Epoch 74/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.4519\n","Epoch 75/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.4393\n","Epoch 76/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.4266\n","Epoch 77/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.4140\n","Epoch 78/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.4014\n","Epoch 79/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.3888\n","Epoch 80/100\n","1/1 [==============================] - 0s 11ms/step - loss: 0.3764\n","Epoch 81/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.3640\n","Epoch 82/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.3517\n","Epoch 83/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.3394\n","Epoch 84/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.3274\n","Epoch 85/100\n","1/1 [==============================] - 0s 12ms/step - loss: 0.3154\n","Epoch 86/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.3036\n","Epoch 87/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2919\n","Epoch 88/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2804\n","Epoch 89/100\n","1/1 [==============================] - 0s 7ms/step - loss: 0.2690\n","Epoch 90/100\n","1/1 [==============================] - 0s 11ms/step - loss: 0.2579\n","Epoch 91/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2469\n","Epoch 92/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2361\n","Epoch 93/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2256\n","Epoch 94/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.2152\n","Epoch 95/100\n","1/1 [==============================] - 0s 10ms/step - loss: 0.2051\n","Epoch 96/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.1952\n","Epoch 97/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.1856\n","Epoch 98/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.1763\n","Epoch 99/100\n","1/1 [==============================] - 0s 6ms/step - loss: 0.1672\n","Epoch 100/100\n","1/1 [==============================] - 0s 10ms/step - loss: 0.1583\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Embedding, Dot, Reshape\n","\n","# Sample corpus\n","corpus = [\n","    \"the cat in the hat\",\n","    \"the quick brown fox\",\n","    \"the lazy dog\",\n","    # Add more sentences as needed\n","]\n","\n","# Tokenize words\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Generate word pairs for context and target words\n","def generate_word_pairs(corpus, window_size=1):\n","    word_pairs = []\n","    for sentence in corpus:\n","        words = tokenizer.texts_to_sequences([sentence])[0]\n","        for i, target_word in enumerate(words):\n","            for context_word in words[max(0, i - window_size) : i + window_size]:\n","                if context_word != target_word:\n","                    word_pairs.append([target_word, context_word])\n","    return np.array(word_pairs)\n","\n","# Build the GloVe model\n","embedding_size = 50  # Choose an appropriate size for your embeddings\n","context_size = 2  # Context window size\n","\n","input_target = Input(shape=(1,))\n","input_context = Input(shape=(1,))\n","\n","embedding = Embedding(total_words, embedding_size, input_length=1)(input_target)\n","context_embedding = Embedding(total_words, embedding_size, input_length=1)(input_context)\n","\n","dot_product = Dot(axes=2)([embedding, context_embedding])\n","dot_product = Reshape((1,))(dot_product)\n","\n","# Define the GloVe model\n","glove_model = Model(inputs=[input_target, input_context], outputs=dot_product)\n","glove_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n","\n","# Generate training data\n","word_pairs = generate_word_pairs(corpus, window_size=context_size)\n","target = np.array([pair[0] for pair in word_pairs], dtype=\"int32\")\n","context = np.array([pair[1] for pair in word_pairs], dtype=\"int32\")\n","labels = np.array([1.0] * len(word_pairs))\n","\n","# Train the model\n","glove_model.fit([target, context], labels, epochs=100, batch_size=32)\n","\n","# Extract word embeddings\n","word_embeddings = glove_model.get_layer(\"embedding\").get_weights()[0]"]},{"cell_type":"code","source":["# Now, word_embeddings contains the trained GloVe embeddings\n","word_embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvnJ4jF1B5Iv","executionInfo":{"status":"ok","timestamp":1701075988179,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"82ac6c2a-7db9-49d0-d77d-e4fdaded8058"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.03011609,  0.03164754, -0.01227959, -0.01718123, -0.02108562,\n","        -0.04339051, -0.03055278,  0.02313313,  0.02580244,  0.02080562,\n","         0.02674754,  0.01736661,  0.04634222,  0.02139523, -0.03735017,\n","         0.02572913, -0.01494846, -0.0322681 ,  0.04311777,  0.02931016,\n","        -0.00747167, -0.04543829,  0.04021717,  0.04912562, -0.01601272,\n","         0.00941927,  0.00574072,  0.01799511, -0.01849099,  0.01693786,\n","        -0.00949676,  0.0082594 ,  0.0120337 , -0.03889183,  0.00913298,\n","         0.01177325,  0.02934283,  0.04777331,  0.03583412, -0.02616826,\n","         0.01232226,  0.03500963, -0.04925743,  0.03835196, -0.01760975,\n","        -0.02751219, -0.04651996,  0.0348387 , -0.01815857, -0.04568649],\n","       [-0.13559745,  0.13986742,  0.09990193, -0.02728223, -0.16652799,\n","        -0.10410953,  0.10235941, -0.11882525,  0.13646148,  0.09205011,\n","         0.15423088, -0.13670278,  0.11939931,  0.10564298, -0.14911763,\n","        -0.16938724,  0.15442142,  0.10704798,  0.16037163, -0.11131185,\n","        -0.12112981, -0.11738084,  0.14015874, -0.15223126,  0.15391329,\n","         0.14183487, -0.11505135,  0.1162683 , -0.08307496,  0.12118861,\n","         0.14915636,  0.14954105,  0.10461583,  0.11644858,  0.08718924,\n","         0.13985723, -0.12490056, -0.06064502,  0.13727891, -0.11223114,\n","        -0.15250792, -0.15971738,  0.10271975, -0.14162458,  0.1683922 ,\n","         0.15837696,  0.1650399 , -0.15990724,  0.12750906, -0.1271096 ],\n","       [-0.14928666,  0.11549943,  0.13652483,  0.1122572 , -0.16353552,\n","         0.06756092,  0.15149362, -0.10322514,  0.11409543,  0.14993636,\n","        -0.16653559,  0.12831257,  0.15037337,  0.16811304, -0.12538877,\n","        -0.14168267,  0.14784168, -0.02596062,  0.10879837,  0.16144928,\n","        -0.00637459, -0.09580443,  0.12715404,  0.1295661 ,  0.09505559,\n","         0.07303328, -0.08666128, -0.11936042,  0.11307866,  0.12823392,\n","        -0.15479355, -0.01673919,  0.12028623, -0.15988426,  0.10652097,\n","         0.09353482, -0.1325447 , -0.13804424,  0.03679692, -0.09645607,\n","        -0.12888972, -0.15713723,  0.10717132, -0.05092543,  0.0903785 ,\n","        -0.126365  ,  0.08868318, -0.11563236,  0.11157608,  0.08008594],\n","       [-0.05398462,  0.10063525,  0.09435897,  0.13003835, -0.08685909,\n","        -0.11975079,  0.10285149, -0.11055315,  0.12769684,  0.14537671,\n","        -0.13148291, -0.13177173,  0.13314189,  0.15935098, -0.05783899,\n","        -0.12205986,  0.16101977,  0.05166857,  0.15644976,  0.06475307,\n","        -0.06328317,  0.10754517,  0.10821843,  0.15821095,  0.1055588 ,\n","         0.16484162, -0.13778499,  0.13577685, -0.08469302,  0.10543764,\n","         0.0118636 ,  0.03097769, -0.00687477, -0.05477075,  0.09929983,\n","         0.11274143, -0.08701902, -0.15425731, -0.1436499 , -0.0908851 ,\n","        -0.14456367, -0.10637022,  0.16245557,  0.1529198 ,  0.14021994,\n","         0.01743805,  0.09755721, -0.13406254,  0.13485044,  0.03270819],\n","       [-0.10581668,  0.06642379,  0.14099973,  0.1284709 , -0.14354283,\n","         0.12187631,  0.11381733, -0.12684487,  0.11688402,  0.08547553,\n","        -0.15994108,  0.09126251,  0.08730674,  0.16720426, -0.06623451,\n","        -0.15735058,  0.0977344 ,  0.0072201 ,  0.12852992,  0.1485314 ,\n","        -0.0511241 , -0.1039098 ,  0.08653402,  0.11501122,  0.13110934,\n","         0.0850236 , -0.11263465, -0.14466435,  0.14319384,  0.14531551,\n","        -0.09980932,  0.01311503,  0.10384103, -0.10779047,  0.13128261,\n","         0.14562358, -0.15375236, -0.11673122,  0.12141839, -0.08093201,\n","        -0.15670142, -0.14567378,  0.13730432, -0.07121039,  0.14135228,\n","        -0.16755448,  0.06696989, -0.16736384,  0.08898936,  0.03889303],\n","       [ 0.06167478,  0.18143122,  0.15850656,  0.08186644,  0.13607688,\n","        -0.13321047,  0.10388091, -0.09620838, -0.04573585,  0.13581364,\n","        -0.12310304, -0.08470585,  0.166814  , -0.10072432, -0.14401948,\n","        -0.12509018,  0.13053872, -0.12981224,  0.08659475,  0.0637208 ,\n","        -0.14259185,  0.11728773,  0.09304056,  0.06920902,  0.15111835,\n","         0.12219233, -0.14923672,  0.02637286,  0.08731576,  0.08371727,\n","        -0.14463459,  0.15800303,  0.08970069, -0.1354738 ,  0.15364186,\n","         0.15237518, -0.1160382 , -0.05909456, -0.11623694, -0.11899624,\n","         0.1196941 ,  0.13400733,  0.12668717,  0.15373766,  0.12000068,\n","        -0.1679968 , -0.06258024, -0.15814327,  0.12036532, -0.11381164],\n","       [ 0.10436802,  0.10990199, -0.11780477,  0.15640703, -0.1157776 ,\n","        -0.15569198, -0.1211462 , -0.131815  , -0.12154003,  0.1562483 ,\n","        -0.15114327, -0.14297253,  0.07965443, -0.11998425, -0.10342474,\n","        -0.12345421, -0.11272462,  0.15126173,  0.08407181,  0.15619662,\n","        -0.11085679, -0.07015147,  0.15286592,  0.14220907,  0.14291863,\n","         0.16144556, -0.16637716,  0.08690059,  0.15353201,  0.07581947,\n","         0.07474918,  0.12488483, -0.09805993, -0.16358756,  0.12854078,\n","         0.09060557, -0.11482012, -0.09794067,  0.13304305, -0.08098911,\n","         0.16412818, -0.02000841,  0.14118275,  0.15445597,  0.11321583,\n","        -0.1115113 , -0.15541066, -0.11469325,  0.12451593,  0.11238744],\n","       [ 0.14701939,  0.09219493,  0.13443534,  0.1640097 ,  0.11895956,\n","        -0.08420773, -0.00986594, -0.16933559, -0.13446514,  0.12009328,\n","        -0.12681052, -0.09009045,  0.16882697, -0.11820769, -0.1476168 ,\n","        -0.08507994, -0.16626933,  0.07983944, -0.10574045,  0.08760214,\n","        -0.11623067, -0.16420974, -0.09908992, -0.09937419,  0.03309499,\n","         0.03376168, -0.08982676,  0.10104119,  0.10559034,  0.15835604,\n","         0.16193692,  0.0777239 ,  0.13856304, -0.06784928,  0.08918408,\n","         0.11776643, -0.13354227,  0.13008447,  0.08114162, -0.16514295,\n","         0.12497151,  0.10447165,  0.1506413 ,  0.1601158 ,  0.15311189,\n","        -0.15515092, -0.13807982, -0.09152925,  0.14847316, -0.13727178],\n","       [-0.1578617 , -0.0880269 ,  0.03773213,  0.11614536, -0.13551867,\n","        -0.11661079, -0.07461704, -0.12336148,  0.12762047,  0.12027603,\n","        -0.14340085, -0.08403795,  0.15616316,  0.12490605, -0.06687973,\n","        -0.12550215,  0.08490773, -0.1449506 ,  0.15150264,  0.07833527,\n","        -0.12604691,  0.15832184,  0.14200738,  0.12792186,  0.1336452 ,\n","         0.12157423, -0.17200334,  0.06216861,  0.04967196,  0.13173231,\n","        -0.12085041, -0.14238986, -0.14507969, -0.158827  ,  0.07846771,\n","         0.14149582, -0.12169001, -0.16383608, -0.12587965, -0.09840837,\n","        -0.16179223, -0.08984502,  0.11040035,  0.12425224,  0.08800991,\n","         0.09204735,  0.1720962 , -0.11165774,  0.11889309,  0.11723846],\n","       [-0.16628991,  0.10368244,  0.17689967, -0.12706532, -0.05418001,\n","        -0.11017081,  0.12413961, -0.16026768,  0.1649861 ,  0.12189689,\n","        -0.11956696, -0.1293089 ,  0.16588734,  0.11122655, -0.14193958,\n","        -0.09299742,  0.04163875, -0.14050482,  0.15331616,  0.05758605,\n","        -0.06465355,  0.13246   ,  0.12628125,  0.10325813,  0.15141447,\n","         0.16896272,  0.09746276,  0.04036867,  0.11243758,  0.17767821,\n","         0.13163665,  0.10335749,  0.07127485, -0.15444438, -0.14909956,\n","         0.05795486, -0.13591999, -0.16849722, -0.10564224, -0.13677143,\n","        -0.08176927, -0.09129749,  0.07925633, -0.11735853,  0.0864902 ,\n","        -0.07967173,  0.14512126, -0.1143282 ,  0.09439498,  0.07484388]],\n","      dtype=float32)"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Sample words for interpretation\n","sample_words = [\"the\", \"cat\", \"in\", \"hat\", \"quick\", \"brown\", \"fox\", \"lazy\", \"dog\"]\n","\n","# Create a dictionary to store word embeddings\n","word_embedding_dict = {}\n","for word in sample_words:\n","    word_index = tokenizer.word_index[word]\n","    word_embedding = word_embeddings[word_index]\n","    word_embedding_dict[word] = word_embedding\n","\n","# Print the word embeddings\n","for word, embedding in word_embedding_dict.items():\n","    print(f\"{word}: {embedding}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDBM4voCCZ68","executionInfo":{"status":"ok","timestamp":1701076118729,"user_tz":-330,"elapsed":449,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"7f1768f1-e043-4ed6-d734-6c59456ccb13"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["the: [-0.13559745  0.13986742  0.09990193 -0.02728223 -0.16652799 -0.10410953\n","  0.10235941 -0.11882525  0.13646148  0.09205011  0.15423088 -0.13670278\n","  0.11939931  0.10564298 -0.14911763 -0.16938724  0.15442142  0.10704798\n","  0.16037163 -0.11131185 -0.12112981 -0.11738084  0.14015874 -0.15223126\n","  0.15391329  0.14183487 -0.11505135  0.1162683  -0.08307496  0.12118861\n","  0.14915636  0.14954105  0.10461583  0.11644858  0.08718924  0.13985723\n"," -0.12490056 -0.06064502  0.13727891 -0.11223114 -0.15250792 -0.15971738\n","  0.10271975 -0.14162458  0.1683922   0.15837696  0.1650399  -0.15990724\n","  0.12750906 -0.1271096 ]\n","cat: [-0.14928666  0.11549943  0.13652483  0.1122572  -0.16353552  0.06756092\n","  0.15149362 -0.10322514  0.11409543  0.14993636 -0.16653559  0.12831257\n","  0.15037337  0.16811304 -0.12538877 -0.14168267  0.14784168 -0.02596062\n","  0.10879837  0.16144928 -0.00637459 -0.09580443  0.12715404  0.1295661\n","  0.09505559  0.07303328 -0.08666128 -0.11936042  0.11307866  0.12823392\n"," -0.15479355 -0.01673919  0.12028623 -0.15988426  0.10652097  0.09353482\n"," -0.1325447  -0.13804424  0.03679692 -0.09645607 -0.12888972 -0.15713723\n","  0.10717132 -0.05092543  0.0903785  -0.126365    0.08868318 -0.11563236\n","  0.11157608  0.08008594]\n","in: [-0.05398462  0.10063525  0.09435897  0.13003835 -0.08685909 -0.11975079\n","  0.10285149 -0.11055315  0.12769684  0.14537671 -0.13148291 -0.13177173\n","  0.13314189  0.15935098 -0.05783899 -0.12205986  0.16101977  0.05166857\n","  0.15644976  0.06475307 -0.06328317  0.10754517  0.10821843  0.15821095\n","  0.1055588   0.16484162 -0.13778499  0.13577685 -0.08469302  0.10543764\n","  0.0118636   0.03097769 -0.00687477 -0.05477075  0.09929983  0.11274143\n"," -0.08701902 -0.15425731 -0.1436499  -0.0908851  -0.14456367 -0.10637022\n","  0.16245557  0.1529198   0.14021994  0.01743805  0.09755721 -0.13406254\n","  0.13485044  0.03270819]\n","hat: [-0.10581668  0.06642379  0.14099973  0.1284709  -0.14354283  0.12187631\n","  0.11381733 -0.12684487  0.11688402  0.08547553 -0.15994108  0.09126251\n","  0.08730674  0.16720426 -0.06623451 -0.15735058  0.0977344   0.0072201\n","  0.12852992  0.1485314  -0.0511241  -0.1039098   0.08653402  0.11501122\n","  0.13110934  0.0850236  -0.11263465 -0.14466435  0.14319384  0.14531551\n"," -0.09980932  0.01311503  0.10384103 -0.10779047  0.13128261  0.14562358\n"," -0.15375236 -0.11673122  0.12141839 -0.08093201 -0.15670142 -0.14567378\n","  0.13730432 -0.07121039  0.14135228 -0.16755448  0.06696989 -0.16736384\n","  0.08898936  0.03889303]\n","quick: [ 0.06167478  0.18143122  0.15850656  0.08186644  0.13607688 -0.13321047\n","  0.10388091 -0.09620838 -0.04573585  0.13581364 -0.12310304 -0.08470585\n","  0.166814   -0.10072432 -0.14401948 -0.12509018  0.13053872 -0.12981224\n","  0.08659475  0.0637208  -0.14259185  0.11728773  0.09304056  0.06920902\n","  0.15111835  0.12219233 -0.14923672  0.02637286  0.08731576  0.08371727\n"," -0.14463459  0.15800303  0.08970069 -0.1354738   0.15364186  0.15237518\n"," -0.1160382  -0.05909456 -0.11623694 -0.11899624  0.1196941   0.13400733\n","  0.12668717  0.15373766  0.12000068 -0.1679968  -0.06258024 -0.15814327\n","  0.12036532 -0.11381164]\n","brown: [ 0.10436802  0.10990199 -0.11780477  0.15640703 -0.1157776  -0.15569198\n"," -0.1211462  -0.131815   -0.12154003  0.1562483  -0.15114327 -0.14297253\n","  0.07965443 -0.11998425 -0.10342474 -0.12345421 -0.11272462  0.15126173\n","  0.08407181  0.15619662 -0.11085679 -0.07015147  0.15286592  0.14220907\n","  0.14291863  0.16144556 -0.16637716  0.08690059  0.15353201  0.07581947\n","  0.07474918  0.12488483 -0.09805993 -0.16358756  0.12854078  0.09060557\n"," -0.11482012 -0.09794067  0.13304305 -0.08098911  0.16412818 -0.02000841\n","  0.14118275  0.15445597  0.11321583 -0.1115113  -0.15541066 -0.11469325\n","  0.12451593  0.11238744]\n","fox: [ 0.14701939  0.09219493  0.13443534  0.1640097   0.11895956 -0.08420773\n"," -0.00986594 -0.16933559 -0.13446514  0.12009328 -0.12681052 -0.09009045\n","  0.16882697 -0.11820769 -0.1476168  -0.08507994 -0.16626933  0.07983944\n"," -0.10574045  0.08760214 -0.11623067 -0.16420974 -0.09908992 -0.09937419\n","  0.03309499  0.03376168 -0.08982676  0.10104119  0.10559034  0.15835604\n","  0.16193692  0.0777239   0.13856304 -0.06784928  0.08918408  0.11776643\n"," -0.13354227  0.13008447  0.08114162 -0.16514295  0.12497151  0.10447165\n","  0.1506413   0.1601158   0.15311189 -0.15515092 -0.13807982 -0.09152925\n","  0.14847316 -0.13727178]\n","lazy: [-0.1578617  -0.0880269   0.03773213  0.11614536 -0.13551867 -0.11661079\n"," -0.07461704 -0.12336148  0.12762047  0.12027603 -0.14340085 -0.08403795\n","  0.15616316  0.12490605 -0.06687973 -0.12550215  0.08490773 -0.1449506\n","  0.15150264  0.07833527 -0.12604691  0.15832184  0.14200738  0.12792186\n","  0.1336452   0.12157423 -0.17200334  0.06216861  0.04967196  0.13173231\n"," -0.12085041 -0.14238986 -0.14507969 -0.158827    0.07846771  0.14149582\n"," -0.12169001 -0.16383608 -0.12587965 -0.09840837 -0.16179223 -0.08984502\n","  0.11040035  0.12425224  0.08800991  0.09204735  0.1720962  -0.11165774\n","  0.11889309  0.11723846]\n","dog: [-0.16628991  0.10368244  0.17689967 -0.12706532 -0.05418001 -0.11017081\n","  0.12413961 -0.16026768  0.1649861   0.12189689 -0.11956696 -0.1293089\n","  0.16588734  0.11122655 -0.14193958 -0.09299742  0.04163875 -0.14050482\n","  0.15331616  0.05758605 -0.06465355  0.13246     0.12628125  0.10325813\n","  0.15141447  0.16896272  0.09746276  0.04036867  0.11243758  0.17767821\n","  0.13163665  0.10335749  0.07127485 -0.15444438 -0.14909956  0.05795486\n"," -0.13591999 -0.16849722 -0.10564224 -0.13677143 -0.08176927 -0.09129749\n","  0.07925633 -0.11735853  0.0864902  -0.07967173  0.14512126 -0.1143282\n","  0.09439498  0.07484388]\n"]}]}]}