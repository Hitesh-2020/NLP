{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5jyZ76RX8eVJ8iQPX369x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#To implement Neural Network Bigram Model."],"metadata":{"id":"2tK8LYmxx5n8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7br6uZ5rx35M","executionInfo":{"status":"ok","timestamp":1701089199863,"user_tz":-330,"elapsed":5438,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"8905fc0e-b398-4d2f-ccb2-c6b9cd9be8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 555ms/step - loss: 2.2787 - accuracy: 0.0000e+00\n","Test Loss: 2.278671979904175, Test Accuracy: 0.0\n","1/1 [==============================] - 1s 698ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 32ms/step\n","Generated Text: brown quick quick quick quick quick\n"]}],"source":["import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample data (replace this with your own dataset)\n","corpus = [\n","    \"the quick\",\n","    \"brown fox\",\n","    \"jumps over\",\n","    \"the lazy\",\n","    \"dog\"\n","]\n","\n","# Tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences and labels for bigram model\n","input_sequences = []\n","for line in corpus:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","max_sequence_length = max(len(seq) for seq in input_sequences)\n","padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","X, y = padded_sequences[:, :-1], padded_sequences[:, -1]\n","\n","# Convert labels to one-hot encoding\n","y = np.eye(total_words)[y]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build and train the neural network model\n","model = Sequential()\n","model.add(Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_length-1))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=50, verbose=0)\n","\n","# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n","\n","# Generate text using the trained model\n","seed_text = \"brown\"\n","for _ in range(5):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n","    predicted_word_index = np.argmax(model.predict(token_list), axis=-1)\n","    predicted_word = tokenizer.index_word[predicted_word_index[0]]\n","    seed_text += \" \" + predicted_word\n","\n","print(f'Generated Text: {seed_text}')\n"]},{"cell_type":"markdown","source":["https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij"],"metadata":{"id":"C54d2JwOKveS"}},{"cell_type":"code","source":["\n","def readData():\n","    data = ['This is a  dog','This is a cat','I love my cat','This is my name ']\n","    dat=[]\n","    for i in range(len(data)):\n","        for word in data[i].split():\n","            dat.append(word)\n","    print(dat)\n","    return dat\n","\n","def createBigram(data):\n","   listOfBigrams = []\n","   bigramCounts = {}\n","   unigramCounts = {}\n","   for i in range(len(data)-1):\n","      if i < len(data) - 1 and data[i+1].islower():\n","\n","         listOfBigrams.append((data[i], data[i + 1]))\n","\n","         if (data[i], data[i+1]) in bigramCounts:\n","            bigramCounts[(data[i], data[i + 1])] += 1\n","         else:\n","            bigramCounts[(data[i], data[i + 1])] = 1\n","\n","      if data[i] in unigramCounts:\n","         unigramCounts[data[i]] += 1\n","      else:\n","         unigramCounts[data[i]] = 1\n","   return listOfBigrams, unigramCounts, bigramCounts\n","\n","\n","def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n","    listOfProb = {}\n","    for bigram in listOfBigrams:\n","        word1 = bigram[0]\n","        word2 = bigram[1]\n","        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n","    return listOfProb\n","\n","\n","if __name__ == '__main__':\n","    data = readData()\n","    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n","\n","    print(\"\\n All the possible Bigrams are \")\n","    print(listOfBigrams)\n","\n","    print(\"\\n Bigrams along with their frequency \")\n","    print(bigramCounts)\n","\n","    print(\"\\n Unigrams along with their frequency \")\n","    print(unigramCounts)\n","\n","    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n","\n","    print(\"\\n Bigrams along with their probability \")\n","    print(bigramProb)\n","    inputList=\"This is my cat\"\n","    splt=inputList.split()\n","    outputProb1 = 1\n","    bilist=[]\n","    bigrm=[]\n","\n","    for i in range(len(splt) - 1):\n","        if i < len(splt) - 1:\n","\n","            bilist.append((splt[i], splt[i + 1]))\n","\n","    print(\"\\n The bigrams in given sentence are \")\n","    print(bilist)\n","    for i in range(len(bilist)):\n","        if bilist[i] in bigramProb:\n","\n","            outputProb1 *= bigramProb[bilist[i]]\n","        else:\n","\n","            outputProb1 *= 0\n","    print('\\n' + 'Probablility of sentence \\\"This is my cat\\\" = ' + str(outputProb1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDgIP2ZrIBS5","executionInfo":{"status":"ok","timestamp":1701111828961,"user_tz":-330,"elapsed":5,"user":{"displayName":"Hitesh","userId":"15040887444265747839"}},"outputId":"0ec914a6-2db6-4128-f92c-347740fd17a5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n","\n"," All the possible Bigrams are \n","[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n","\n"," Bigrams along with their frequency \n","{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n","\n"," Unigrams along with their frequency \n","{'This': 3, 'is': 3, 'a': 2, 'dog': 1, 'cat': 2, 'I': 1, 'love': 1, 'my': 2}\n","\n"," Bigrams along with their probability \n","{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n","\n"," The bigrams in given sentence are \n","[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n","\n","Probablility of sentence \"This is my cat\" = 0.16666666666666666\n"]}]}]}